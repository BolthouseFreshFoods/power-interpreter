# Power Interpreter

A production-grade Python execution sandbox exposed as an MCP (Model Context Protocol) server. Designed to exceed the capabilities of standard Code Interpreter environments â€” with multi-session support, direct CDN file loading, async job execution, SQL querying, and a full enterprise analytics library stack.

Deployed on [Railway](https://railway.app) and integrated with SimTheory AI agents via MCP/SSE.

---

## What Makes This Different from Code Interpreter

| Capability | Code Interpreter | Power Interpreter |
|---|---|---|
| pandas / numpy / matplotlib | âœ… | âœ… |
| scikit-learn / xgboost / lightgbm | âœ… | âœ… |
| statsmodels / pingouin | âœ… | âœ… |
| plotly / seaborn / kaleido | âœ… | âœ… |
| sympy (symbolic math) | âœ… | âœ… |
| DuckDB (in-process SQL) | âŒ | âœ… |
| Parquet / Arrow columnar data | âŒ | âœ… |
| PDF reading (pdfplumber) | âŒ | âœ… |
| Multi-session (concurrent) | âŒ One per conversation | âœ… Up to 6 named sessions |
| Execution timeout | ~120s | 120s default, up to 300s |
| File upload via CDN URL | âŒ | âœ… `fetch_from_url` |
| Async long-running jobs | âŒ | âœ… `submit_job` / `get_job_result` |
| Cross-session file isolation | âŒ | âœ… Per `session_id` sandbox |
| Internet access | âœ… | âŒ Sandboxed (by design) |

---

## Architecture

```
SimTheory Agent
      â”‚
      â”‚  MCP/SSE (HTTPS)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         main.py (FastAPI)       â”‚
â”‚   /mcp/sse  â†â”€â”€ MCP endpoint   â”‚
â”‚   /health   â†â”€â”€ Railway probe  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       mcp_server.py (FastMCP)   â”‚
â”‚  12 registered MCP tools        â”‚
â”‚  fetch_from_url â† NEW           â”‚
â”‚  execute_code                   â”‚
â”‚  submit_job / get_job_result    â”‚
â”‚  load_dataset / query_dataset   â”‚
â”‚  upload_file / fetch_file       â”‚
â”‚  list_files / list_datasets     â”‚
â”‚  create_session                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     executor.py (Kernel Mgr)    â”‚
â”‚  Up to 6 concurrent kernels     â”‚
â”‚  Session persistence            â”‚
â”‚  Chart capture (PNG)            â”‚
â”‚  120s timeout (300s max)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   /app/sandbox_data/{session}/  â”‚
â”‚   Isolated per session_id       â”‚
â”‚   Files persist within session  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## MCP Tools Reference (12 Tools)

### ğŸ”´ Priority Tools â€” Start Here

#### `fetch_from_url` â­ NEW
Load any file directly from a CDN URL (Cloudinary, S3, HTTPS) into the sandbox. This is the **primary way to load files** â€” no base64 encoding required.

```json
{
  "tool": "fetch_from_url",
  "args": {
    "url": "https://cdn.simtheory.ai/raw/upload/v.../myfile.xlsx",
    "filename": "myfile.xlsx",
    "session_id": "default"
  }
}
```

Returns the exact sandbox path to use in `execute_code`. Supports: `xlsx, xls, csv, tsv, json, jsonl, parquet, pdf, txt, png, jpg, zip, db, sqlite`. Max file size: 500MB.

---

#### `execute_code`
Run Python in a sandboxed, persistent kernel session.

```json
{
  "tool": "execute_code",
  "args": {
    "code": "import pandas as pd\ndf = pd.read_excel('/app/sandbox_data/default/myfile.xlsx')\nprint(df.describe())",
    "session_id": "default",
    "timeout": 120
  }
}
```

- Variables persist across calls within the same `session_id`
- Charts (matplotlib, seaborn, plotly) are captured and returned as PNG image blocks
- Files written to `/app/sandbox_data/{session_id}/` are accessible to other tools
- Default timeout: 120s. Maximum: 300s.

---

#### `submit_job` + `get_job_status` + `get_job_result`
For long-running tasks (large dataset processing, ML training, complex cross-references). Submit asynchronously and poll for results.

```json
// Submit
{ "tool": "submit_job", "args": { "code": "...", "session_id": "analysis", "timeout": 240 } }
// â†’ Returns: { "job_id": "a3f9c1b2" }

// Poll
{ "tool": "get_job_status", "args": { "job_id": "a3f9c1b2" } }
// â†’ Returns: { "status": "running", "elapsed": "14.2s" }

// Retrieve
{ "tool": "get_job_result", "args": { "job_id": "a3f9c1b2" } }
// â†’ Returns: full output + charts
```

---

### ğŸ“ File Management

#### `upload_file`
Upload a file via base64-encoded content. For large files, prefer `fetch_from_url`.

```json
{
  "tool": "upload_file",
  "args": {
    "filename": "data.csv",
    "content_base64": "<base64 string>",
    "session_id": "default"
  }
}
```

#### `fetch_file`
Retrieve a file generated by `execute_code` from the sandbox (returned as base64).

```json
{ "tool": "fetch_file", "args": { "filename": "results.xlsx", "session_id": "default" } }
```

#### `list_files`
List all files in a sandbox session.

```json
{ "tool": "list_files", "args": { "session_id": "default" } }
```

---

### ğŸ“Š Dataset Tools

#### `load_dataset`
Load a sandbox file into a named pandas DataFrame. Supports xlsx, csv, parquet, json.

```json
{
  "tool": "load_dataset",
  "args": {
    "filename": "invoices.xlsx",
    "dataset_name": "invoices",
    "session_id": "default",
    "sheet_name": "Sheet1"
  }
}
```

#### `query_dataset`
Run SQL against any loaded DataFrame using DuckDB â€” no database setup required.

```json
{
  "tool": "query_dataset",
  "args": {
    "query": "SELECT vendor, SUM(amount) as total FROM invoices GROUP BY vendor ORDER BY total DESC",
    "session_id": "default"
  }
}
```

#### `list_datasets`
List all DataFrames currently loaded in a session (name, shape, columns).

---

### âš™ï¸ Session Management

#### `create_session`
Create a named session with its own isolated sandbox directory and kernel.

```json
{ "tool": "create_session", "args": { "session_id": "vestis_analysis" } }
```

Up to 6 concurrent sessions supported. Each session has:
- Its own kernel with persistent variable state
- Its own `/app/sandbox_data/{session_id}/` file directory
- Independent execution context

---

## Recommended Workflow

### Loading and Analyzing a File from CDN

```
1. fetch_from_url(url="https://cdn.simtheory.ai/.../data.xlsx", session_id="analysis")
   â†’ âœ… File saved to /app/sandbox_data/analysis/data.xlsx (212,789 bytes)

2. execute_code(code="""
   import pandas as pd
   df = pd.read_excel('/app/sandbox_data/analysis/data.xlsx')
   print(df.shape)
   print(df.dtypes)
   print(df.describe())
   """, session_id="analysis")

3. query_dataset(query="SELECT vendor, COUNT(*) as n, SUM(amount) as total FROM df GROUP BY vendor", session_id="analysis")

4. execute_code(code="""
   import matplotlib.pyplot as plt
   df.groupby('vendor')['amount'].sum().plot(kind='bar')
   plt.title('Revenue by Vendor')
   plt.tight_layout()
   plt.savefig('/app/sandbox_data/analysis/chart.png')
   """, session_id="analysis")

5. fetch_file(filename="chart.png", session_id="analysis")
```

---

## Analytics Library Stack

### Core Data Science
- `pandas` â€” DataFrames, time series, data wrangling
- `numpy` â€” Numerical computing
- `scipy` â€” Scientific computing, statistical tests

### Machine Learning
- `scikit-learn` â€” Classification, regression, clustering, preprocessing
- `xgboost` â€” Gradient boosting
- `lightgbm` â€” Fast gradient boosting for large datasets

### Visualization
- `matplotlib` â€” Publication-quality charts
- `seaborn` â€” Statistical visualization
- `plotly` + `kaleido` â€” Interactive charts, static export

### Statistics & Econometrics
- `statsmodels` â€” OLS, time series (ARIMA), hypothesis testing
- `pingouin` â€” Statistical tests (t-test, ANOVA, correlation)

### Symbolic Math
- `sympy` â€” Symbolic algebra, calculus, equation solving

### Data Formats
- `openpyxl` / `xlsxwriter` / `xlrd` â€” Excel read/write
- `pyarrow` / `fastparquet` â€” Parquet / columnar data
- `duckdb` â€” In-process SQL on DataFrames
- `pdfplumber` / `PyPDF2` â€” PDF text extraction
- `python-docx` â€” Word document generation
- `beautifulsoup4` / `lxml` â€” HTML/XML parsing

### Image Processing
- `Pillow` â€” Image manipulation, format conversion

### Utilities
- `rich` â€” Beautiful terminal output
- `tabulate` â€” Table formatting
- `tqdm` â€” Progress bars
- `tenacity` â€” Retry logic

---

## Deployment

### Environment Variables

| Variable | Default | Description |
|---|---|---|
| `PORT` | `8000` | HTTP port (set by Railway) |
| `EXECUTOR_URL` | `http://127.0.0.1:8080` | Internal executor endpoint |
| `SANDBOX_DATA_DIR` | `/app/sandbox_data` | Root sandbox directory |
| `MAX_UPLOAD_MB` | `500` | Max file upload size |
| `MAX_FETCH_SIZE_MB` | `500` | Max fetch_from_url file size |
| `LOG_LEVEL` | `INFO` | Logging verbosity |

### Railway Deployment

Pushes to `main` trigger automatic Railway deployments. Health check is configured in `railway.toml` at `/health`.

```toml
# railway.toml
[deploy]
healthcheckPath = "/health"
healthcheckTimeout = 30
restartPolicyType = "on_failure"
```

---

## Project Structure

```
power-interpreter/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI app, /mcp/sse endpoint, /health
â”‚   â”œâ”€â”€ mcp_server.py        # FastMCP â€” all 12 MCP tools registered here
â”‚   â”œâ”€â”€ fetch_from_url.py    # â˜… NEW â€” CDN/URL file fetcher
â”‚   â”œâ”€â”€ models.py            # Pydantic request/response models
â”‚   â”œâ”€â”€ storage.py           # Sandbox file management
â”‚   â””â”€â”€ engine/
â”‚       â”œâ”€â”€ executor.py      # Python kernel execution, chart capture
â”‚       â””â”€â”€ kernel_manager.py # Session lifecycle, up to 6 kernels
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ railway.toml
â”œâ”€â”€ requirements.txt         # Full analytics library suite
â”œâ”€â”€ start.py                 # Startup script (reads PORT env var)
â””â”€â”€ README.md
```

---

## Recent Changes

### v2.0 â€” Priority 1 & 2 (Feb 2026)

**Priority 1 â€” Full Analytics Library Suite**
- Added `scikit-learn`, `xgboost`, `lightgbm` for ML
- Added `plotly` + `kaleido` for interactive/static charts
- Added `statsmodels`, `pingouin` for advanced statistics
- Added `sympy` for symbolic math
- Added `Pillow` for image processing
- Added `pdfplumber`, `PyPDF2`, `python-docx` for document handling
- Added `duckdb`, `sqlalchemy` for in-process SQL
- Added `pyarrow`, `fastparquet` for columnar data formats
- Added `beautifulsoup4`, `lxml` for HTML/XML parsing
- Added `rich`, `tabulate`, `tqdm`, `tenacity` for utilities

**Priority 2 â€” File Loading via CDN URL**
- Added `fetch_from_url` tool â€” streams files directly from any HTTPS URL into sandbox
- Eliminates base64 upload bottleneck for large files
- Supports 500MB max file size with 64KB streaming chunks
- Sanitizes filenames, validates extensions, guards against path traversal
- Execution timeout increased from 30s â†’ 120s default (300s max)
- `mcp_server.py` fully rewritten with clean tool registration and docstrings
- `load_dataset` now supports xlsx, csv, parquet, json natively
- `query_dataset` uses DuckDB for SQL on any loaded DataFrame

---

## License

MIT
